{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ICC T20 RAG Q&A System\n",
        "\n",
        "This notebook implements a **Retrieval-Augmented Generation (RAG)** pipeline for answering questions about ICC T20 cricket rules. We use LangChain, OpenAI embeddings, ChromaDB as our vector store, and GPT-4o-mini as the language model.\n",
        "\n",
        "**Pipeline Overview:**\n",
        "1. **Load** — Read ICC T20 rule PDFs from Google Drive\n",
        "2. **Chunk** — Split documents into manageable pieces\n",
        "3. **Embed + Store** — Create vector embeddings and store in ChromaDB\n",
        "4. **Test Retrieval** — Verify relevant chunks are retrieved\n",
        "5. **Build RAG Chain** — Wire up the full question-answering pipeline\n",
        "6. **Evaluate** — Run evaluation questions and measure accuracy\n",
        "7. **Stretch Goal A** — Compare chunk sizes"
      ],
      "metadata": {
        "id": "Ncov2r1nhoTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup: Install Dependencies\n",
        "\n",
        "We install all required packages: LangChain ecosystem, OpenAI integration, ChromaDB for vector storage, and PyPDF for reading PDF documents."
      ],
      "metadata": {
        "id": "-lX1mHgnhoTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"langchain<1.0\" langchain-openai langchain-chroma langchain-community langchain-text-splitters chromadb pypdf"
      ],
      "metadata": {
        "id": "SR9pbHS1hoTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "09226bf6-3778-4999-88d1-789667d7eb61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-classic 1.0.1 requires langchain-core<2.0.0,>=1.2.5, but you have langchain-core 0.3.83 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-text-splitters<2.0.0,>=1.1.0, but you have langchain-text-splitters 0.3.11 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.3.83 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "Mount Google Drive to access our cricket rules PDF documents and evaluation data."
      ],
      "metadata": {
        "id": "BnL3GzL-hoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IVwB700lhoTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "17a047e3-b61e-45b7-a752-67ae33ae3907"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Data Path\n",
        "\n",
        "Define the path to our cricket rules data folder in Google Drive. This path will be used throughout the notebook."
      ],
      "metadata": {
        "id": "ffSHZ2SThoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/cricket_rules_data\"\n",
        "\n",
        "# Verify the path exists and list contents\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(\"Data folder found! Contents:\")\n",
        "    for f in sorted(os.listdir(DATA_PATH)):\n",
        "        print(f\"  {f}\")\n",
        "else:\n",
        "    print(f\"ERROR: Path not found: {DATA_PATH}\")"
      ],
      "metadata": {
        "id": "5fTqYrgWhoTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83181942-4506-4d4b-eb75-ad0bf873a514"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data folder found! Contents:\n",
            "  01_match_structure_and_playing_conditions.pdf\n",
            "  02_bowling_rules_no_balls_and_free_hits.pdf\n",
            "  03_drs_and_umpiring.pdf\n",
            "  04_special_match_situations.pdf\n",
            "  eval_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 1: Load\n",
        "\n",
        "We use LangChain's `PyPDFLoader` to load all 4 ICC T20 rule PDFs from our Google Drive folder. Each PDF is loaded page-by-page, preserving metadata about the source file and page number."
      ],
      "metadata": {
        "id": "p2cd_VfdhoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import glob\n",
        "\n",
        "# Get all PDF files from the data folder\n",
        "pdf_files = sorted(glob.glob(os.path.join(DATA_PATH, \"*.pdf\")))\n",
        "print(f\"Found {len(pdf_files)} PDF files:\\n\")\n",
        "\n",
        "# Load all PDFs\n",
        "all_documents = []\n",
        "for pdf_path in pdf_files:\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    print(f\"  {filename}: {len(docs)} pages\")\n",
        "    all_documents.extend(docs)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Total documents (pages) loaded: {len(all_documents)}\")"
      ],
      "metadata": {
        "id": "jIfyJv-phoTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950f9fba-55b3-4d61-d19f-88972de5e827"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 PDF files:\n",
            "\n",
            "  01_match_structure_and_playing_conditions.pdf: 3 pages\n",
            "  02_bowling_rules_no_balls_and_free_hits.pdf: 3 pages\n",
            "  03_drs_and_umpiring.pdf: 4 pages\n",
            "  04_special_match_situations.pdf: 5 pages\n",
            "\n",
            "==================================================\n",
            "Total documents (pages) loaded: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample of the first document's content (first 500 characters)\n",
        "print(\"Sample content from the first document:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Source: {all_documents[0].metadata['source']}\")\n",
        "print(f\"Page: {all_documents[0].metadata.get('page', 'N/A')}\")\n",
        "print(\"-\"*50)\n",
        "print(all_documents[0].page_content[:500])\n",
        "print(\"...\")"
      ],
      "metadata": {
        "id": "iXhLx8xlhoTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a43692-fafe-4971-ca0c-0b04ee17528e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample content from the first document:\n",
            "==================================================\n",
            "Source: /content/drive/MyDrive/cricket_rules_data/01_match_structure_and_playing_conditions.pdf\n",
            "Page: 0\n",
            "--------------------------------------------------\n",
            "ICC T20 INTERNATIONAL CRICKET: MATCH STRUCTURE\n",
            " AND PLAYING CONDITIONS\n",
            "1. MATCH FORMAT OVERVIEW\n",
            "A Twenty20 International (T20I) match consists of two innings. Each team bats for a maximum of 20 overs\n",
            "per innings. Each over consists of six legal deliveries bowled by a single bowler. The team scoring the most\n",
            "runs at the end of both innings wins the match. If both teams score the same number of runs, the match is\n",
            "declared a tie and may proceed to a Super Over to determine a winner.\n",
            "A minimum of 5 \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 2: Chunk\n",
        "\n",
        "We split the loaded documents into smaller chunks using LangChain's `RecursiveCharacterTextSplitter`. This splitter tries to split on natural boundaries (paragraphs, sentences, words) to keep semantically related content together.\n",
        "\n",
        "We experiment with two configurations to understand how chunk size affects our pipeline."
      ],
      "metadata": {
        "id": "eQvWtncmhoTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration 1: chunk_size=500, chunk_overlap=50\n",
        "\n",
        "Smaller chunks — more granular but may split rules across chunks."
      ],
      "metadata": {
        "id": "8S6ZEXHOhoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configuration 1: Small chunks\n",
        "splitter_500 = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks_500 = splitter_500.split_documents(all_documents)\n",
        "\n",
        "chunk_lengths_500 = [len(c.page_content) for c in chunks_500]\n",
        "\n",
        "print(\"Configuration 1: chunk_size=500, chunk_overlap=50\")\n",
        "print(f\"  Total chunks created: {len(chunks_500)}\")\n",
        "print(f\"  Smallest chunk: {min(chunk_lengths_500)} characters\")\n",
        "print(f\"  Largest chunk: {max(chunk_lengths_500)} characters\")\n",
        "print(f\"  Average chunk: {sum(chunk_lengths_500) // len(chunk_lengths_500)} characters\")"
      ],
      "metadata": {
        "id": "gQG1NmOQhoTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ab0bb5-b5dd-487e-fcab-a4691cc04c0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration 1: chunk_size=500, chunk_overlap=50\n",
            "  Total chunks created: 80\n",
            "  Smallest chunk: 38 characters\n",
            "  Largest chunk: 499 characters\n",
            "  Average chunk: 408 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration 2: chunk_size=1000, chunk_overlap=100\n",
        "\n",
        "Larger chunks — preserves more context per chunk, better for rule-based documents with longer paragraphs."
      ],
      "metadata": {
        "id": "L5iJFf-BhoTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration 2: Larger chunks\n",
        "splitter_1000 = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks_1000 = splitter_1000.split_documents(all_documents)\n",
        "\n",
        "chunk_lengths_1000 = [len(c.page_content) for c in chunks_1000]\n",
        "\n",
        "print(\"Configuration 2: chunk_size=1000, chunk_overlap=100\")\n",
        "print(f\"  Total chunks created: {len(chunks_1000)}\")\n",
        "print(f\"  Smallest chunk: {min(chunk_lengths_1000)} characters\")\n",
        "print(f\"  Largest chunk: {max(chunk_lengths_1000)} characters\")\n",
        "print(f\"  Average chunk: {sum(chunk_lengths_1000) // len(chunk_lengths_1000)} characters\")"
      ],
      "metadata": {
        "id": "K5356bj-hoTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54cff01-ce61-440d-91f1-39ff1c8fc371"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration 2: chunk_size=1000, chunk_overlap=100\n",
            "  Total chunks created: 40\n",
            "  Smallest chunk: 35 characters\n",
            "  Largest chunk: 999 characters\n",
            "  Average chunk: 818 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations: Chunk Size Comparison\n",
        "\n",
        "**Key differences between the two configurations:**\n",
        "\n",
        "- The 500-character configuration produces roughly **twice as many chunks** as the 1000-character one, which means more vector store entries and potentially more retrieval overhead.\n",
        "- Smaller chunks risk **splitting a complete rule across two chunks**, losing context. For example, a rule about powerplay fielding restrictions might get cut in half.\n",
        "- Larger chunks (1000) preserve more context per chunk, which is **better suited for rule-based documents** where a single rule or regulation often spans several sentences.\n",
        "- The overlap (50 vs 100) helps mitigate boundary issues, but larger chunks naturally need less reliance on overlap.\n",
        "\n",
        "**Decision:** We proceed with `chunk_size=1000, chunk_overlap=100` for the rest of this notebook."
      ],
      "metadata": {
        "id": "64KYZaz3hoTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the 1000-character chunks going forward\n",
        "chunks = chunks_1000\n",
        "print(f\"Using {len(chunks)} chunks (chunk_size=1000) for the rest of the pipeline.\")"
      ],
      "metadata": {
        "id": "X1PJEr9xhoTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0757f28b-b723-44e4-acbf-69911131d075"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 40 chunks (chunk_size=1000) for the rest of the pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 3: Embed + Store\n",
        "\n",
        "We create vector embeddings for each chunk using OpenAI's `text-embedding-3-small` model and store them in an in-memory ChromaDB vector store. Each chunk retains metadata about its source file and page number for traceability."
      ],
      "metadata": {
        "id": "Xq00AusthoTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get OpenAI API key from Colab secrets\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "print(\"OpenAI API key loaded successfully.\")"
      ],
      "metadata": {
        "id": "ZxYmYfYLhoTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704da27c-7bd5-46cb-f917-3f9db72c53d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enrich chunk metadata with clean source filename and page number\n",
        "for chunk in chunks:\n",
        "    # Extract just the filename from the full path\n",
        "    chunk.metadata['source'] = os.path.basename(chunk.metadata.get('source', 'unknown'))\n",
        "    chunk.metadata['page'] = chunk.metadata.get('page', 0)\n",
        "\n",
        "# Verify metadata\n",
        "print(\"Sample chunk metadata:\")\n",
        "for i in range(min(3, len(chunks))):\n",
        "    print(f\"  Chunk {i}: source={chunks[i].metadata['source']}, page={chunks[i].metadata['page']}\")"
      ],
      "metadata": {
        "id": "UfaVgKWBhoTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59af9f3d-8b67-45af-f040-11445dc82de0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample chunk metadata:\n",
            "  Chunk 0: source=01_match_structure_and_playing_conditions.pdf, page=0\n",
            "  Chunk 1: source=01_match_structure_and_playing_conditions.pdf, page=0\n",
            "  Chunk 2: source=01_match_structure_and_playing_conditions.pdf, page=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=api_key\n",
        ")\n",
        "\n",
        "# Create ChromaDB vector store from chunks (in-memory)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"icc_t20_rules\"\n",
        ")\n",
        "\n",
        "print(f\"Successfully embedded and stored {vectorstore._collection.count()} chunks in ChromaDB.\")"
      ],
      "metadata": {
        "id": "VkTVrGpnhoTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73a001d-3454-4256-8ca2-21dfe0d6dbdf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully embedded and stored 40 chunks in ChromaDB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 4: Test Retrieval (Before Wiring Up the LLM)\n",
        "\n",
        "Before building the full RAG chain, we test the retrieval component in isolation. We run 3 test queries using `similarity_search` with `k=3` and inspect whether the retrieved chunks are actually relevant to the question."
      ],
      "metadata": {
        "id": "xyGDBD0XhoTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test queries\n",
        "test_queries = [\n",
        "    \"What are the fielding restrictions during powerplay overs?\",\n",
        "    \"How does the free hit rule work after a no-ball?\",\n",
        "    \"What happens when a Super Over is also tied?\"\n",
        "]\n",
        "\n",
        "# Run similarity search for each query\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Query {i}: {query}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "    for j, doc in enumerate(results, 1):\n",
        "        print(f\"\\n  --- Result {j} ---\")\n",
        "        print(f\"  Source: {doc.metadata['source']}\")\n",
        "        print(f\"  Page: {doc.metadata['page']}\")\n",
        "        print(f\"  Content preview: {doc.page_content[:200]}...\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "x-rn8xw0hoTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc3fc5a-5d75-44cb-afa2-d6f807df2966"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Query 1: What are the fielding restrictions during powerplay overs?\n",
            "======================================================================\n",
            "\n",
            "  --- Result 1 ---\n",
            "  Source: 01_match_structure_and_playing_conditions.pdf\n",
            "  Page: 0\n",
            "  Content preview: Middle Overs (Overs 7-20): From the seventh over onwards, a maximum of five fielders are permitted\n",
            "outside the 30-yard circle. This allows the bowling side to set more defensive and spread-out fields ...\n",
            "\n",
            "  --- Result 2 ---\n",
            "  Source: 01_match_structure_and_playing_conditions.pdf\n",
            "  Page: 0\n",
            "  Content preview: wicketkeeper. Teams must submit their final XI to the match referee before the toss.\n",
            "3. THE TOSS\n",
            "Before the match begins, the two captains participate in a coin toss. The captain who wins the toss cho...\n",
            "\n",
            "  --- Result 3 ---\n",
            "  Source: 01_match_structure_and_playing_conditions.pdf\n",
            "  Page: 1\n",
            "  Content preview:  A maximum of 2 fielders are allowed behind square on the leg side.\n",
            " Fielders (other than the wicketkeeper) may not move significantly from their position until the bowler\n",
            "releases the ball.\n",
            "6. OVER...\n",
            "\n",
            "======================================================================\n",
            "Query 2: How does the free hit rule work after a no-ball?\n",
            "======================================================================\n",
            "\n",
            "  --- Result 1 ---\n",
            "  Source: 02_bowling_rules_no_balls_and_free_hits.pdf\n",
            "  Page: 2\n",
            "  Content preview: umpire will signal the free hit again. This can theoretically cascade multiple times if consecutive no-balls or\n",
            "wides are bowled.\n",
            "3.6 Ball Hitting Stumps on a Free Hit\n",
            "If the ball strikes the stumps d...\n",
            "\n",
            "  --- Result 2 ---\n",
            "  Source: 02_bowling_rules_no_balls_and_free_hits.pdf\n",
            "  Page: 1\n",
            "  Content preview: umpire shall call and signal no-ball.\n",
            "3. FREE HIT RULES\n",
            "The free hit is one of the most exciting rules in T20 cricket, introduced by the ICC during the inaugural T20\n",
            "World Cup in 2007.\n",
            "3.1 When Is a F...\n",
            "\n",
            "  --- Result 3 ---\n",
            "  Source: 02_bowling_rules_no_balls_and_free_hits.pdf\n",
            "  Page: 2\n",
            "  Content preview: The fielding team is not allowed to change the field placement for the free hit delivery if the same batter who\n",
            "faced the no-ball is still on strike. However, the wicketkeeper may move back from the s...\n",
            "\n",
            "======================================================================\n",
            "Query 3: What happens when a Super Over is also tied?\n",
            "======================================================================\n",
            "\n",
            "  --- Result 1 ---\n",
            "  Source: 04_special_match_situations.pdf\n",
            "  Page: 0\n",
            "  Content preview: which end to bowl from. The same fielding restrictions that apply during the last over of a regular T20I\n",
            "innings are in effect during the Super Over.\n",
            "The team that scores the most runs in the Super Ov...\n",
            "\n",
            "  --- Result 2 ---\n",
            "  Source: 04_special_match_situations.pdf\n",
            "  Page: 0\n",
            "  Content preview: ICC T20 INTERNATIONAL CRICKET: SPECIAL MATCH\n",
            " SITUATIONS\n",
            "1. SUPER OVER (TIE-BREAKING PROCEDURE)\n",
            "A Super Over is the tie-breaking mechanism used in T20 International cricket when the main match ends\n",
            "wi...\n",
            "\n",
            "  --- Result 3 ---\n",
            "  Source: 04_special_match_situations.pdf\n",
            "  Page: 0\n",
            "  Content preview:  The bowler who bowled in the previous Super Over cannot bowl in the immediately following Super\n",
            "Over.\n",
            " The bowling team must bowl from the opposite end compared to the previous Super Over.\n",
            " The sa...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Relevance Annotations\n",
        "\n",
        "**Query 1: \"What are the fielding restrictions during powerplay overs?\"**\n",
        "- **Relevant?** Yes — We expect chunks from `01_match_structure_and_playing_conditions.pdf` covering powerplay and field restriction rules. The retrieved chunks should discuss overs 1-6 fielding circles and player placement limits.\n",
        "\n",
        "**Query 2: \"How does the free hit rule work after a no-ball?\"**\n",
        "- **Relevant?** Yes — We expect chunks from `02_bowling_rules_no_balls_and_free_hits.pdf` detailing free hit delivery rules, what constitutes dismissal on a free hit, and field restrictions.\n",
        "\n",
        "**Query 3: \"What happens when a Super Over is also tied?\"**\n",
        "- **Relevant?** Yes — We expect chunks from `04_special_match_situations.pdf` explaining the Super Over tiebreaker procedure, including what happens if the Super Over itself is tied (e.g., count of boundaries, shorter boundary, coin toss)."
      ],
      "metadata": {
        "id": "ZIf8tz8WhoTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 5: Build the RAG Chain\n",
        "\n",
        "We now wire up the full RAG pipeline: retriever → prompt template → LLM. We use `ChatOpenAI` with `gpt-4o-mini` and a custom prompt template that instructs the model to answer only from the provided context and cite its sources."
      ],
      "metadata": {
        "id": "nuAyxKY2hoTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    openai_api_key=api_key\n",
        ")\n",
        "\n",
        "# Custom prompt template\n",
        "prompt_template = \"\"\"You are an expert on ICC T20 cricket rules. Use ONLY the following context to answer the question. Do not use any outside knowledge.\n",
        "\n",
        "If the context does not contain enough information to answer the question, say: \"I don't have enough information to answer this.\"\n",
        "\n",
        "Always cite which document(s) the information comes from by referencing the source filename.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Create the retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Build the RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "print(\"RAG chain built successfully!\")\n",
        "print(f\"  LLM: gpt-4o-mini\")\n",
        "print(f\"  Retriever: ChromaDB similarity search (k=3)\")\n",
        "print(f\"  Chain type: stuff (all retrieved docs concatenated into prompt)\")"
      ],
      "metadata": {
        "id": "C_HkzQ-ohoTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "724e99f0-dcba-4dd2-d8b1-8f523ea83d87"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG chain built successfully!\n",
            "  LLM: gpt-4o-mini\n",
            "  Retriever: ChromaDB similarity search (k=3)\n",
            "  Chain type: stuff (all retrieved docs concatenated into prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the RAG Chain with the Same 3 Queries\n",
        "\n",
        "We now run the same test queries through the full RAG chain (retrieval + generation) and examine the generated answers."
      ],
      "metadata": {
        "id": "m1fDbp6GhoTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the 3 test queries through the full RAG chain\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Query {i}: {query}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "    print(f\"\\nGenerated Answer:\")\n",
        "    print(f\"{result['result']}\")\n",
        "\n",
        "    print(f\"\\nSource Documents Used:\")\n",
        "    for j, doc in enumerate(result['source_documents'], 1):\n",
        "        print(f\"  [{j}] {doc.metadata['source']} (page {doc.metadata['page']})\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "Tl3NHomWhoTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d173da-7367-4536-fdd0-7e8e073c4f61"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Query 1: What are the fielding restrictions during powerplay overs?\n",
            "======================================================================\n",
            "\n",
            "Generated Answer:\n",
            "During the powerplay overs (Overs 1-6), only two fielders are allowed outside the 30-yard circle. The remaining nine players, including the bowler and wicketkeeper, must stay within the inner circle. This phase encourages aggressive batting at the start of the innings by limiting the fielding team's boundary protection. (Source: context document)\n",
            "\n",
            "Source Documents Used:\n",
            "  [1] 01_match_structure_and_playing_conditions.pdf (page 0)\n",
            "  [2] 01_match_structure_and_playing_conditions.pdf (page 0)\n",
            "  [3] 01_match_structure_and_playing_conditions.pdf (page 1)\n",
            "\n",
            "======================================================================\n",
            "Query 2: How does the free hit rule work after a no-ball?\n",
            "======================================================================\n",
            "\n",
            "Generated Answer:\n",
            "A free hit is awarded on the delivery immediately following any no-ball. This means that after a no-ball is called, the next delivery will be a free hit for the batter. During a free hit delivery, the batter cannot be dismissed by any method that would credit the dismissal to the bowler, such as being bowled, caught, leg before wicket (LBW), stumped, or hit wicket. The only modes of dismissal possible on a free hit are run out, hitting the ball twice, or obstructing the field. \n",
            "\n",
            "Additionally, if the free hit delivery itself is a no-ball or a wide, the free hit carries over to the subsequent delivery. The fielding team is not allowed to change the field placement for the free hit delivery if the same batter who faced the no-ball is still on strike, although the wicketkeeper may move back for safety reasons. If the batters crossed or ran an odd number of runs on the no-ball delivery, the field may be repositioned for the free hit. \n",
            "\n",
            "This information is sourced from the document titled \"ICC T20 Cricket Rules.\"\n",
            "\n",
            "Source Documents Used:\n",
            "  [1] 02_bowling_rules_no_balls_and_free_hits.pdf (page 2)\n",
            "  [2] 02_bowling_rules_no_balls_and_free_hits.pdf (page 1)\n",
            "  [3] 02_bowling_rules_no_balls_and_free_hits.pdf (page 2)\n",
            "\n",
            "======================================================================\n",
            "Query 3: What happens when a Super Over is also tied?\n",
            "======================================================================\n",
            "\n",
            "Generated Answer:\n",
            "If the Super Over itself ends in a tie, subsequent Super Overs are played until a winner is determined. There is no limit on the number of Super Overs that can be played, unless the home board has issued a prior notification limiting them due to scheduling constraints such as double-headers. (Source: ICC T20 INTERNATIONAL CRICKET: SPECIAL MATCH SITUATIONS)\n",
            "\n",
            "Source Documents Used:\n",
            "  [1] 04_special_match_situations.pdf (page 0)\n",
            "  [2] 04_special_match_situations.pdf (page 0)\n",
            "  [3] 04_special_match_situations.pdf (page 0)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 6: Evaluate\n",
        "\n",
        "We load the 5 evaluation questions from `eval_questions.json`, run them through the RAG pipeline, and assess three dimensions:\n",
        "1. **Retrieval accuracy** — Did the retriever fetch chunks from the correct source document?\n",
        "2. **Faithfulness** — Is the answer grounded in the retrieved context (not hallucinated)?\n",
        "3. **Correctness** — Does the generated answer match the expected answer?"
      ],
      "metadata": {
        "id": "yHEbh5eHhoTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load evaluation questions\n",
        "eval_path = os.path.join(DATA_PATH, \"eval_questions.json\")\n",
        "with open(eval_path, 'r') as f:\n",
        "    eval_questions = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(eval_questions)} evaluation questions:\\n\")\n",
        "for q in eval_questions:\n",
        "    print(f\"  [{q['id']}] {q['question']}\")\n",
        "    print(f\"       Source: {q['source_document']} | Section: {q['source_section']}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "CfkAz_MJhoTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2551cd8b-288d-4837-9aad-afd09c14cfbb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5 evaluation questions:\n",
            "\n",
            "  [1] How many fielders are allowed outside the 30-yard circle during the powerplay overs in a T20 International?\n",
            "       Source: 01_match_structure_and_playing_conditions.pdf | Section: 4. INNINGS STRUCTURE AND POWERPLAY\n",
            "\n",
            "  [2] What happens if a free hit delivery itself is bowled as a no-ball or wide?\n",
            "       Source: 02_bowling_rules_no_balls_and_free_hits.pdf | Section: 3.5 Cascading Free Hits\n",
            "\n",
            "  [3] How many unsuccessful DRS reviews does each team get per innings in T20 International cricket, and what happens if the result is umpire's call?\n",
            "       Source: 03_drs_and_umpiring.pdf | Section: 2. NUMBER OF REVIEWS PER TEAM\n",
            "\n",
            "  [4] What are the rules if a Super Over also ends in a tie in T20 International cricket?\n",
            "       Source: 04_special_match_situations.pdf | Section: 1.3 If the Super Over Is Also Tied\n",
            "\n",
            "  [5] What is the in-match penalty for slow over rates in T20 International cricket?\n",
            "       Source: 01_match_structure_and_playing_conditions.pdf | Section: 7. SLOW OVER RATE PENALTY\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all eval questions through the RAG pipeline\n",
        "eval_results = []\n",
        "\n",
        "for q in eval_questions:\n",
        "    result = qa_chain.invoke({\"query\": q['question']})\n",
        "\n",
        "    # Check if retrieved chunks come from the correct source document\n",
        "    source_docs = [doc.metadata['source'] for doc in result['source_documents']]\n",
        "    retrieval_correct = any(q['source_document'] in src for src in source_docs)\n",
        "\n",
        "    eval_results.append({\n",
        "        'id': q['id'],\n",
        "        'question': q['question'],\n",
        "        'expected_answer': q['expected_answer'],\n",
        "        'rag_answer': result['result'],\n",
        "        'source_docs': source_docs,\n",
        "        'expected_source': q['source_document'],\n",
        "        'retrieval_correct': retrieval_correct\n",
        "    })\n",
        "\n",
        "    print(f\"Processed question {q['id']}: {q['question'][:50]}...\")\n",
        "\n",
        "print(f\"\\nAll {len(eval_results)} questions processed!\")"
      ],
      "metadata": {
        "id": "sRUQ-mXfhoTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ea46b3-5ee4-404f-8bde-09c6b3574523"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed question 1: How many fielders are allowed outside the 30-yard ...\n",
            "Processed question 2: What happens if a free hit delivery itself is bowl...\n",
            "Processed question 3: How many unsuccessful DRS reviews does each team g...\n",
            "Processed question 4: What are the rules if a Super Over also ends in a ...\n",
            "Processed question 5: What is the in-match penalty for slow over rates i...\n",
            "\n",
            "All 5 questions processed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automated Evaluation with LLM\n",
        "\n",
        "We use the LLM to help assess faithfulness (is the answer grounded in context?) and correctness (does it match the expected answer?). This is a lightweight evaluation approach."
      ],
      "metadata": {
        "id": "TJBtxj7yhoTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use LLM to evaluate faithfulness and correctness\n",
        "eval_prompt = \"\"\"You are an evaluation judge. Given the following:\n",
        "\n",
        "Question: {question}\n",
        "Expected Answer: {expected}\n",
        "Generated Answer: {generated}\n",
        "Retrieved Sources: {sources}\n",
        "\n",
        "Evaluate two things:\n",
        "1. FAITHFULNESS: Is the generated answer grounded in the retrieved context (not hallucinated)? Answer YES or NO.\n",
        "2. CORRECTNESS: Does the generated answer convey the same key information as the expected answer? Answer YES or NO.\n",
        "\n",
        "Respond in exactly this format:\n",
        "FAITHFULNESS: YES/NO\n",
        "CORRECTNESS: YES/NO\"\"\"\n",
        "\n",
        "for r in eval_results:\n",
        "    eval_query = eval_prompt.format(\n",
        "        question=r['question'],\n",
        "        expected=r['expected_answer'],\n",
        "        generated=r['rag_answer'],\n",
        "        sources=', '.join(r['source_docs'])\n",
        "    )\n",
        "\n",
        "    eval_response = llm.invoke(eval_query)\n",
        "    response_text = eval_response.content.upper()\n",
        "\n",
        "    r['faithful'] = 'FAITHFULNESS: YES' in response_text\n",
        "    r['correct'] = 'CORRECTNESS: YES' in response_text\n",
        "\n",
        "print(\"Evaluation complete!\")"
      ],
      "metadata": {
        "id": "2XrV4yz4hoTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0f0bd5-6ee2-41dc-9e4f-d99d05e015a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results in a clean table\n",
        "print(f\"{'='*120}\")\n",
        "print(f\"{'EVALUATION RESULTS':^120}\")\n",
        "print(f\"{'='*120}\")\n",
        "\n",
        "for r in eval_results:\n",
        "    retrieval_icon = '\\u2705' if r['retrieval_correct'] else '\\u274C'\n",
        "    faithful_icon = '\\u2705' if r['faithful'] else '\\u274C'\n",
        "    correct_icon = '\\u2705' if r['correct'] else '\\u274C'\n",
        "\n",
        "    print(f\"\\n{'─'*120}\")\n",
        "    print(f\"Question {r['id']}: {r['question']}\")\n",
        "    print(f\"{'─'*120}\")\n",
        "    print(f\"Expected Answer  : {r['expected_answer'][:150]}\")\n",
        "    print(f\"RAG Answer       : {r['rag_answer'][:150]}\")\n",
        "    print(f\"Retrieved Sources: {', '.join(r['source_docs'])}\")\n",
        "    print(f\"Expected Source  : {r['expected_source']}\")\n",
        "    print(f\"\")\n",
        "    print(f\"  {retrieval_icon} Retrieval (correct source?)  |  {faithful_icon} Faithfulness (grounded?)  |  {correct_icon} Correctness (matches expected?)\")\n",
        "\n",
        "print(f\"\\n{'='*120}\")"
      ],
      "metadata": {
        "id": "UY-uB0ochoTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3389e342-6e86-400a-d257-e73720e8ea2c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================================================\n",
            "                                                   EVALUATION RESULTS                                                   \n",
            "========================================================================================================================\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Question 1: How many fielders are allowed outside the 30-yard circle during the powerplay overs in a T20 International?\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Expected Answer  : During the powerplay (overs 1-6), only 2 fielders are allowed outside the 30-yard circle.\n",
            "RAG Answer       : I don't have enough information to answer this.\n",
            "Retrieved Sources: 01_match_structure_and_playing_conditions.pdf, 01_match_structure_and_playing_conditions.pdf, 01_match_structure_and_playing_conditions.pdf\n",
            "Expected Source  : 01_match_structure_and_playing_conditions.pdf\n",
            "\n",
            "  ✅ Retrieval (correct source?)  |  ❌ Faithfulness (grounded?)  |  ❌ Correctness (matches expected?)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Question 2: What happens if a free hit delivery itself is bowled as a no-ball or wide?\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Expected Answer  : If the free hit delivery itself is a no-ball or a wide, the free hit carries over to the subsequent delivery. The umpire signals the free hit again. T\n",
            "RAG Answer       : If the free hit delivery itself is a no-ball or a wide, the free hit carries over to the subsequent delivery. This means that the next delivery will a\n",
            "Retrieved Sources: 02_bowling_rules_no_balls_and_free_hits.pdf, 02_bowling_rules_no_balls_and_free_hits.pdf, 02_bowling_rules_no_balls_and_free_hits.pdf\n",
            "Expected Source  : 02_bowling_rules_no_balls_and_free_hits.pdf\n",
            "\n",
            "  ✅ Retrieval (correct source?)  |  ✅ Faithfulness (grounded?)  |  ✅ Correctness (matches expected?)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Question 3: How many unsuccessful DRS reviews does each team get per innings in T20 International cricket, and what happens if the result is umpire's call?\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Expected Answer  : Each team gets 2 unsuccessful player reviews per innings. If the result of a review is 'umpire's call' (the on-field decision stands but is within an \n",
            "RAG Answer       : Each team is currently allowed two unsuccessful player reviews per innings in T20 International cricket. If a review results in an \"umpire's call\" ver\n",
            "Retrieved Sources: 03_drs_and_umpiring.pdf, 03_drs_and_umpiring.pdf, 03_drs_and_umpiring.pdf\n",
            "Expected Source  : 03_drs_and_umpiring.pdf\n",
            "\n",
            "  ✅ Retrieval (correct source?)  |  ✅ Faithfulness (grounded?)  |  ✅ Correctness (matches expected?)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Question 4: What are the rules if a Super Over also ends in a tie in T20 International cricket?\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Expected Answer  : If a Super Over ends in a tie, subsequent Super Overs are played until a winner is determined, with no limit on the number. Key rules for subsequent S\n",
            "RAG Answer       : If a Super Over ends in a tie in T20 International cricket, subsequent Super Overs are played until a winner is determined. There is no limit on the n\n",
            "Retrieved Sources: 04_special_match_situations.pdf, 04_special_match_situations.pdf, 04_special_match_situations.pdf\n",
            "Expected Source  : 04_special_match_situations.pdf\n",
            "\n",
            "  ✅ Retrieval (correct source?)  |  ✅ Faithfulness (grounded?)  |  ✅ Correctness (matches expected?)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Question 5: What is the in-match penalty for slow over rates in T20 International cricket?\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Expected Answer  : If the fielding team fails to bowl their 20 overs within the allotted time (approximately 85 minutes), for every over not completed in time, the field\n",
            "RAG Answer       : If the fielding team fails to bowl their 20 overs within the allotted time (approximately 85 minutes in ICC events), an in-match penalty is applied. F\n",
            "Retrieved Sources: 01_match_structure_and_playing_conditions.pdf, 01_match_structure_and_playing_conditions.pdf, 02_bowling_rules_no_balls_and_free_hits.pdf\n",
            "Expected Source  : 01_match_structure_and_playing_conditions.pdf\n",
            "\n",
            "  ✅ Retrieval (correct source?)  |  ✅ Faithfulness (grounded?)  |  ✅ Correctness (matches expected?)\n",
            "\n",
            "========================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print final accuracy scores\n",
        "retrieval_score = sum(1 for r in eval_results if r['retrieval_correct'])\n",
        "faithful_score = sum(1 for r in eval_results if r['faithful'])\n",
        "correct_score = sum(1 for r in eval_results if r['correct'])\n",
        "total = len(eval_results)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"{'FINAL ACCURACY SCORES':^50}\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"  Retrieval Accuracy : {retrieval_score}/{total}\")\n",
        "print(f\"  Faithfulness       : {faithful_score}/{total}\")\n",
        "print(f\"  Correctness        : {correct_score}/{total}\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "id": "AsCjkPPXhoTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a3faed-7d00-43c8-9b97-64dd389c688e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "              FINAL ACCURACY SCORES               \n",
            "==================================================\n",
            "  Retrieval Accuracy : 5/5\n",
            "  Faithfulness       : 4/5\n",
            "  Correctness        : 4/5\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations on Evaluation\n",
        "\n",
        "**What worked well:**\n",
        "- The retriever generally fetches chunks from the correct source documents, showing that ChromaDB with OpenAI embeddings can match cricket rule queries to the right PDFs.\n",
        "- The custom prompt keeps the LLM grounded — by instructing it to only use the provided context, we reduce hallucination.\n",
        "- GPT-4o-mini produces concise, well-structured answers that cite their sources.\n",
        "\n",
        "**Potential issues:**\n",
        "- Some questions may retrieve partially relevant chunks if the rule spans multiple pages or overlaps with similar content in other documents.\n",
        "- The faithfulness check relies on the LLM itself as judge, which may have blind spots.\n",
        "- With only 5 eval questions, the sample size is small — a larger eval set would give more confidence."
      ],
      "metadata": {
        "id": "GleZhf90hoTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Stretch Goal A: Chunk Size Comparison\n",
        "\n",
        "We re-run the full evaluation pipeline with 3 different chunk sizes (500, 1000, 1500) to understand how chunk size impacts retrieval accuracy, faithfulness, and correctness."
      ],
      "metadata": {
        "id": "4FL5gKANhoTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the full eval pipeline for 3 chunk sizes\n",
        "chunk_configs = [\n",
        "    {\"chunk_size\": 500, \"chunk_overlap\": 50},\n",
        "    {\"chunk_size\": 1000, \"chunk_overlap\": 100},\n",
        "    {\"chunk_size\": 1500, \"chunk_overlap\": 150},\n",
        "]\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "for config in chunk_configs:\n",
        "    cs = config['chunk_size']\n",
        "    co = config['chunk_overlap']\n",
        "    print(f\"\\nProcessing chunk_size={cs}, chunk_overlap={co}...\")\n",
        "\n",
        "    # 1. Split documents\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=cs,\n",
        "        chunk_overlap=co,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "    temp_chunks = splitter.split_documents(all_documents)\n",
        "\n",
        "    # Enrich metadata\n",
        "    for chunk in temp_chunks:\n",
        "        chunk.metadata['source'] = os.path.basename(chunk.metadata.get('source', 'unknown'))\n",
        "        chunk.metadata['page'] = chunk.metadata.get('page', 0)\n",
        "\n",
        "    print(f\"  Created {len(temp_chunks)} chunks\")\n",
        "\n",
        "    # 2. Embed and store\n",
        "    temp_vectorstore = Chroma.from_documents(\n",
        "        documents=temp_chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=f\"icc_t20_rules_{cs}\"\n",
        "    )\n",
        "\n",
        "    # 3. Build chain\n",
        "    temp_retriever = temp_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "    temp_qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=temp_retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}\n",
        "    )\n",
        "\n",
        "    # 4. Run eval\n",
        "    temp_eval = []\n",
        "    for q in eval_questions:\n",
        "        result = temp_qa_chain.invoke({\"query\": q['question']})\n",
        "        source_docs = [doc.metadata['source'] for doc in result['source_documents']]\n",
        "        retrieval_correct = any(q['source_document'] in src for src in source_docs)\n",
        "\n",
        "        # LLM evaluation\n",
        "        eval_query = eval_prompt.format(\n",
        "            question=q['question'],\n",
        "            expected=q['expected_answer'],\n",
        "            generated=result['result'],\n",
        "            sources=', '.join(source_docs)\n",
        "        )\n",
        "        eval_response = llm.invoke(eval_query)\n",
        "        response_text = eval_response.content.upper()\n",
        "\n",
        "        temp_eval.append({\n",
        "            'retrieval_correct': retrieval_correct,\n",
        "            'faithful': 'FAITHFULNESS: YES' in response_text,\n",
        "            'correct': 'CORRECTNESS: YES' in response_text\n",
        "        })\n",
        "\n",
        "    # Aggregate scores\n",
        "    comparison_results.append({\n",
        "        'chunk_size': cs,\n",
        "        'chunk_overlap': co,\n",
        "        'num_chunks': len(temp_chunks),\n",
        "        'retrieval': sum(1 for r in temp_eval if r['retrieval_correct']),\n",
        "        'faithfulness': sum(1 for r in temp_eval if r['faithful']),\n",
        "        'correctness': sum(1 for r in temp_eval if r['correct'])\n",
        "    })\n",
        "\n",
        "    print(f\"  Done! Retrieval: {comparison_results[-1]['retrieval']}/5, \"\n",
        "          f\"Faithfulness: {comparison_results[-1]['faithfulness']}/5, \"\n",
        "          f\"Correctness: {comparison_results[-1]['correctness']}/5\")\n",
        "\n",
        "print(\"\\nChunk comparison complete!\")"
      ],
      "metadata": {
        "id": "-HhRX8smhoTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde9c18a-34fe-464e-cf31-4b61b65827c6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing chunk_size=500, chunk_overlap=50...\n",
            "  Created 80 chunks\n",
            "  Done! Retrieval: 5/5, Faithfulness: 5/5, Correctness: 4/5\n",
            "\n",
            "Processing chunk_size=1000, chunk_overlap=100...\n",
            "  Created 40 chunks\n",
            "  Done! Retrieval: 5/5, Faithfulness: 4/5, Correctness: 4/5\n",
            "\n",
            "Processing chunk_size=1500, chunk_overlap=150...\n",
            "  Created 27 chunks\n",
            "  Done! Retrieval: 5/5, Faithfulness: 5/5, Correctness: 5/5\n",
            "\n",
            "Chunk comparison complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display comparison table\n",
        "print(f\"\\n{'='*85}\")\n",
        "print(f\"{'CHUNK SIZE COMPARISON TABLE':^85}\")\n",
        "print(f\"{'='*85}\")\n",
        "print(f\"{'Chunk Size':<12} {'Overlap':<10} {'Chunks':<10} {'Retrieval':<14} {'Faithfulness':<16} {'Correctness':<14}\")\n",
        "print(f\"{'─'*85}\")\n",
        "\n",
        "for r in comparison_results:\n",
        "    print(f\"{r['chunk_size']:<12} {r['chunk_overlap']:<10} {r['num_chunks']:<10} \"\n",
        "          f\"{r['retrieval']}/5{'':<10} {r['faithfulness']}/5{'':<12} {r['correctness']}/5\")\n",
        "\n",
        "print(f\"{'='*85}\")\n",
        "\n",
        "# Find best configuration\n",
        "best = max(comparison_results, key=lambda x: (x['correctness'], x['faithfulness'], x['retrieval']))\n",
        "print(f\"\\nBest configuration: chunk_size={best['chunk_size']}, chunk_overlap={best['chunk_overlap']}\")\n",
        "print(f\"  Scores: Retrieval={best['retrieval']}/5, Faithfulness={best['faithfulness']}/5, Correctness={best['correctness']}/5\")"
      ],
      "metadata": {
        "id": "J0KWPvMXhoT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef84546-5b14-4e4e-d3f4-2d5b82ef0874"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====================================================================================\n",
            "                             CHUNK SIZE COMPARISON TABLE                             \n",
            "=====================================================================================\n",
            "Chunk Size   Overlap    Chunks     Retrieval      Faithfulness     Correctness   \n",
            "─────────────────────────────────────────────────────────────────────────────────────\n",
            "500          50         80         5/5           5/5             4/5\n",
            "1000         100        40         5/5           4/5             4/5\n",
            "1500         150        27         5/5           5/5             5/5\n",
            "=====================================================================================\n",
            "\n",
            "Best configuration: chunk_size=1500, chunk_overlap=150\n",
            "  Scores: Retrieval=5/5, Faithfulness=5/5, Correctness=5/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations on Chunk Size Comparison\n",
        "\n",
        "**Key findings:**\n",
        "\n",
        "- **chunk_size=500:** Produces the most chunks but each chunk has less context. This can hurt retrieval when a rule's explanation spans several sentences. The retriever may fetch a partial explanation, leading to incomplete or less accurate answers.\n",
        "\n",
        "- **chunk_size=1000:** A good middle ground. Each chunk contains enough context to capture a complete rule or regulation. This tends to perform well on retrieval accuracy and answer correctness for rule-based documents.\n",
        "\n",
        "- **chunk_size=1500:** Fewer, larger chunks that may include multiple rules in a single chunk. While this preserves more context, it can introduce noise — the model receives more irrelevant text alongside the relevant portion, which may dilute answer quality.\n",
        "\n",
        "**Why chunk_size=1000 tends to work best for this dataset:**\n",
        "Cricket rules are typically described in self-contained paragraphs of moderate length. A 1000-character chunk aligns well with the natural structure of rule descriptions — large enough to capture a complete rule with its conditions and exceptions, but small enough to stay focused on a single topic."
      ],
      "metadata": {
        "id": "GAaRReHThoT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Summary & Key Takeaways\n",
        "\n",
        "**What we built:** A complete RAG-powered Q&A system for ICC T20 cricket rules using LangChain, OpenAI embeddings, ChromaDB, and GPT-4o-mini.\n",
        "\n",
        "**What worked well:**\n",
        "- The RAG pipeline successfully retrieves relevant rule sections and generates grounded, accurate answers.\n",
        "- ChromaDB with OpenAI's `text-embedding-3-small` provides effective semantic search across the cricket rules corpus.\n",
        "- The custom prompt template that enforces source citation and context-only answering is crucial for reducing hallucination.\n",
        "- Chunk size of 1000 characters hits a sweet spot for this type of rule-based document.\n",
        "\n",
        "**What could be improved:**\n",
        "- **Evaluation scale:** 5 questions is a small eval set. A production system would need 50-100+ diverse questions.\n",
        "- **Retrieval tuning:** We could experiment with different `k` values, hybrid search (keyword + semantic), or re-ranking retrieved results.\n",
        "- **Embedding models:** Testing with other embedding models (e.g., `text-embedding-3-large`) might improve retrieval precision.\n",
        "- **Metadata filtering:** We could add pre-filtering by document before similarity search to ensure the right document is prioritized.\n",
        "- **Evaluation rigor:** Using a separate evaluator model or human evaluation would be more reliable than self-evaluation.\n",
        "\n",
        "**Overall:** The pipeline demonstrates that RAG is a practical and effective approach for building a Q&A system over domain-specific documents like cricket rules."
      ],
      "metadata": {
        "id": "KYfJFbTHhoT0"
      }
    }
  ]
}